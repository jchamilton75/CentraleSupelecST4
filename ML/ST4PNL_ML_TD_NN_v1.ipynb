{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eT7-MMpfrlHR"
   },
   "source": [
    "# Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QdmaHjz-xCm-"
   },
   "outputs": [],
   "source": [
    "COLAB=True # toggle accordingly\n",
    "#COLAB=False\n",
    "if COLAB:\n",
    "    import numpy as np\n",
    "    print (np.__version__)\n",
    "    !pip install numpy==1.15.4 # to avoid bug loading hdf file \"ValueError: cannot set WRITEABLE flag to True of this array\"\n",
    "    print (np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IsChk6u-r7fk"
   },
   "outputs": [],
   "source": [
    "#### Reading file from Google Drive\n",
    "if COLAB:\n",
    "    !pip install PyDrive\n",
    "    import os\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kl-4W4Ifs8EV"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # download dataset \n",
    "    #directory download = drive.CreateFile({'id': '1TuvyEky9lCcc5ybC6nTl5zzkIm8p8AVO'})\n",
    "    #download.GetContentFile(\"dataWW\")\n",
    "\n",
    "    #download three file \n",
    "    download = drive.CreateFile({'id': '1RN6BVBH4A3v_BBKguiEDJQ2z_pLNWHL_'})\n",
    "    download.GetContentFile(\"dataWW_qq.hdf\")\n",
    "    download = drive.CreateFile({'id': '12kRnrvsluiBvXqvNOk2lb0Agna13lSzV'})\n",
    "    download.GetContentFile(\"dataWW_ggH.hdf\")\n",
    "    download = drive.CreateFile({'id': '1nsGp13Mw3CzjtON---j1cWQziuFpkoEp'})\n",
    "    download.GetContentFile(\"dataWW_VBFH.hdf\")\n",
    "    \n",
    "    !ls -lrt\n",
    "    datapath=\"\"\n",
    "else:\n",
    "    datapath=\"/Users/rousseau/Google Drive/GD_ST4Perso/CoursML/TDML/dataWW/\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MDGbfWaTrlHS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from IPython import display\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline\n",
    "import time\n",
    "pd.set_option('display.max_columns', None) # to see all columns of df.head()\n",
    "np.random.seed(31415) # set the random seed\n",
    "\n",
    "#!pip install pytables # to read hdf5\n",
    "#!pip install xgboost\n",
    "#!pip install lightgbm # not sure it loads the .so\n",
    "#!conda install --yes lightgbm # better run in separate window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVJFiNmgrlHU"
   },
   "outputs": [],
   "source": [
    "# some utilities\n",
    "from math import sqrt\n",
    "from math import log\n",
    "\n",
    "\n",
    "def amsasimov(s,b): # asimove significance\n",
    "        if b<=0 or s<=0:\n",
    "            return 0\n",
    "        try:\n",
    "            return sqrt(2*((s+b)*log(1+float(s)/b)-s))\n",
    "        except ValueError:\n",
    "            print(1+float(s)/b)\n",
    "            print (2*((s+b)*log(1+float(s)/b)-s))\n",
    "        #return s/sqrt(s+b)\n",
    "\n",
    "#\n",
    "def compare_train_test(y_pred_train, y_train, y_pred, y_test, high_low=(0,1), bins=30, xlabel=\"\", ylabel=\"Arbitrary units\", title=\"\", weights_train=np.array([]), weights_test=np.array([])):\n",
    "    if weights_train.size != 0:\n",
    "        weights_train_signal = weights_train[y_train == 1]\n",
    "        weights_train_background = weights_train[y_train == 0]\n",
    "    else:\n",
    "        weights_train_signal = None\n",
    "        weights_train_background = None\n",
    "    plt.hist(y_pred_train[y_train == 1],\n",
    "                 color='r', alpha=0.5, range=high_low, bins=bins,\n",
    "                 histtype='stepfilled', normed=True,\n",
    "                 label='S (train)', weights=weights_train_signal) # alpha is transparancy\n",
    "    plt.hist(y_pred_train[y_train == 0],\n",
    "                 color='b', alpha=0.5, range=high_low, bins=bins,\n",
    "                 histtype='stepfilled', normed=True,\n",
    "                 label='B (train)', weights=weights_train_background)\n",
    "\n",
    "    if weights_test.size != 0:\n",
    "        weights_test_signal = weights_test[y_test == 1]\n",
    "        weights_test_background = weights_test[y_test == 0]\n",
    "    else:\n",
    "        weights_test_signal = None\n",
    "        weights_test_background = None\n",
    "    hist, bins = np.histogram(y_pred[y_test == 1],\n",
    "                                  bins=bins, range=high_low, normed=True, weights=weights_test_signal)\n",
    "    scale = len(y_pred[y_test == 1]) / sum(hist)\n",
    "    err = np.sqrt(hist * scale) / scale\n",
    "\n",
    "    #width = (bins[1] - bins[0])\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.errorbar(center, hist, yerr=err, fmt='o', c='r', label='S (test)')\n",
    "\n",
    "    hist, bins = np.histogram(y_pred[y_test == 0],\n",
    "                                  bins=bins, range=high_low, normed=True, weights=weights_test_background)\n",
    "    scale = len(y_pred[y_test == 0]) / sum(hist)\n",
    "    err = np.sqrt(hist * scale) / scale\n",
    "\n",
    "    #width = (bins[1] - bins[0])\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.errorbar(center, hist, yerr=err, fmt='o', c='b', label='B (test)')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBqFUzeXrlHX"
   },
   "outputs": [],
   "source": [
    "#load signal, background data\n",
    "#more info on dataset http://opendata.atlas.cern/books/current/openatlasdatatools/_book/simulated_data_details.html\n",
    "# dataset ID qq : 105985, ggH : 160155, vbfh : 160205\n",
    "# cross section  qq: 12.42 pb, ggH: 13.17 pb , VBFH : 1.617 pb\n",
    "# unfortunately cross section in the document are wrong\n",
    "qq_cross_section=1242 # deliberately wrong to increase background level\n",
    "ggh_cross_section=13.17*0.5 # deliberately wrong to \n",
    "vbfh_cross_section=1.617*0.5 # deliberately wrong to \n",
    "\n",
    "\n",
    "\n",
    "# total integrated luminosity ATLAS 2012 : fb^-1\n",
    "luminosity=21 \n",
    "# even more info in http://opendata.atlas.cern/books/current/openatlasdatatools/_book/glossary.html \n",
    "\n",
    "#if fail to load, might be due to numpy version, see first cell\n",
    "qq_events = pd.read_hdf(datapath+\"dataWW_qq.hdf\",\"qq\",WRITEABLE=False ) #quarks (qq background) http://opendata.cern.ch/record/3800\n",
    "ggh_events = pd.read_hdf(datapath+\"dataWW_ggH.hdf\",\"ggH\",WRITEABLE=False ) # gluons fusion http://opendata.cern.ch/record/3825\n",
    "vbfh_events = pd.read_hdf(datapath+\"dataWW_VBFH.hdf\",\"VBFH\",WRITEABLE=False ) #Vector Boson Fusion http://opendata.cern.ch/record/3826\n",
    "vbfh_events.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "xgfAFqAqrlHX"
   },
   "source": [
    "# load signal, background data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OzfF0gNlrlHb"
   },
   "outputs": [],
   "source": [
    "nvbfkeep=int(ggh_events.shape[0]*0.1232)\n",
    "print (\"keep only \",nvbfkeep, \" vbfh events to respect vbf H/ gg H ratio\")\n",
    "vbfh_events=vbfh_events[0:nvbfkeep]\n",
    "\n",
    "\n",
    "qq_events[\"class\"] = 0   # background category\n",
    "ggh_events[\"class\"] = 1  # signal category\n",
    "vbfh_events[\"class\"] = 1  # signal category\n",
    "h_events = pd.concat([ggh_events, vbfh_events]) # merge the two higgs dataset\n",
    "del ggh_events  # avoid mistakes later\n",
    "del vbfh_events  # avoid mistakes later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9l7wkkorlHe"
   },
   "outputs": [],
   "source": [
    "h_events.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oz-lWJhgrlHg"
   },
   "outputs": [],
   "source": [
    "h_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQsalTmorlHj"
   },
   "outputs": [],
   "source": [
    "\n",
    "vbfh_cross_section=1.617\n",
    "h_cross_section=ggh_cross_section+vbfh_cross_section\n",
    "\n",
    "\n",
    "print (\"before normalisation\")\n",
    "# in principle should multiply all the scaleFactor (then weights will vary event by event)\n",
    "class_weights = (qq_events.mcWeight.sum(), h_events.mcWeight.sum()) \n",
    "print(\"total class weights\",class_weights)\n",
    "\n",
    "\n",
    "class_nevents = (len(qq_events.index), len(h_events.index))\n",
    "print (\"total class number of events\",class_nevents)\n",
    "\n",
    "qq_weight=qq_cross_section*luminosity/qq_events.shape[0]*qq_events.mcWeight\n",
    "qq_events[\"weight\"]=qq_weight\n",
    "h_weight=h_cross_section*luminosity/h_events.shape[0]*h_events.mcWeight\n",
    "h_events[\"weight\"]=h_weight\n",
    "\n",
    "print (\"after normalisation\")\n",
    "# in principle should multiply all the scaleFactor (then weights will vary event by event)\n",
    "total_weights = (qq_events.weight.sum(), h_events.weight.sum()) \n",
    "\n",
    "print(\"total total weights\",total_weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nnlsmXOrlHn"
   },
   "outputs": [],
   "source": [
    "fulldata = pd.concat([qq_events, h_events]) # merge the datasets\n",
    "fulldata = fulldata.sample(frac=1).reset_index(drop=True) #shuffle the events\n",
    "fulldata.head(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rtI5u5GErlHq"
   },
   "source": [
    "## Event selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kaO2JM1hrlHr"
   },
   "outputs": [],
   "source": [
    "print (fulldata.shape)\n",
    "fulldata=fulldata[fulldata.lep_n==2]\n",
    "print (fulldata.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2CrdDqZgrlHu"
   },
   "outputs": [],
   "source": [
    "fulldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5tCZ6cTrlHw"
   },
   "outputs": [],
   "source": [
    "#DR replace dummy -999 by -10 because of XGboost hist bug\n",
    "fulldata.replace(-999,-7,inplace=True)\n",
    "fulldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ksUkHZprlHy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iMQpKhDKrlH0"
   },
   "outputs": [],
   "source": [
    "#hide class in separate vector\n",
    "#WARNING : there should be no selection nor shuffling later on !\n",
    "target = fulldata[\"class\"]\n",
    "del fulldata[\"class\"]\n",
    "\n",
    "#hide weight in separate vector\n",
    "weights = fulldata[\"weight\"]\n",
    "del fulldata[\"weight\"]\n",
    "fulldata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nviyIMgerlH3"
   },
   "source": [
    "# DO NOT MODIFY ANYTHING ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6e0Hlpv6rlH4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# always rerun exercises from this cell\n",
    "# for simplicity of the exercise only keep some features\n",
    "data=fulldata[[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_phi_0', 'lep_phi_1','jet_n','jet_eta_0', 'jet_eta_1']]\n",
    "#data=fulldata[[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_eta_0', 'lep_eta_1', 'lep_phi_0', 'lep_phi_1','jet_n','jet_pt_0',\n",
    "#       'jet_pt_1', 'jet_eta_0', 'jet_eta_1', 'jet_phi_0', 'jet_phi_1']]\n",
    "print (data.shape)\n",
    "data.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2mf1bLVrlH7"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "data[target==0].hist(figsize=(15,12),color='b')\n",
    "data[target==1].hist(figsize=(15,12),color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ZM-E5H4rlH-"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mvGal8xwzKUY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LzNVhZidzKwr"
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UBSy1alNrlH_"
   },
   "outputs": [],
   "source": [
    "\n",
    "#data[\"lep_deltaphi\"]=np.abs(np.mod(data[\"lep_phi_0\"]-data[\"lep_phi_1\"]+3*np.pi,2*np.pi)-np.pi)\n",
    "#data[\"lep_deltaphi\"]=data[\"lep_phi_0\"]-data[\"lep_phi_1\"]\n",
    "\n",
    "print (data.shape)\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kowHjX4rlIC"
   },
   "source": [
    "## Transformation of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9j5hdrmrlID"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "train_size = 0.75 # fraction of sample used for training\n",
    "X_train, X_test, y_train, y_test, weights_train, weights_test = \\\n",
    "    train_test_split(data, target, weights, train_size=train_size)\n",
    "#reset index for dataseries, not needed for ndarray (X_train, X_test)\n",
    "y_train, y_test, weights_train, weights_test = \\\n",
    "    y_train.reset_index(drop=True),y_test.reset_index(drop=True), \\\n",
    "    weights_train.reset_index(drop=True), weights_test.reset_index(drop=True)\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print (weights_train.shape)\n",
    "print (X_test.shape)\n",
    "print (y_test.shape)\n",
    "print (weights_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#scale to mean 0 and variance 1\n",
    "scaler = StandardScaler()\n",
    "#scaler.fit(data)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)  # applied the transformation calculated the line above\n",
    "\n",
    "\n",
    "class_weights_train = (weights_train[y_train == 0].sum(), weights_train[y_train == 1].sum())\n",
    "print (\"class_weights_train:\",class_weights_train)\n",
    "for i in range(len(class_weights_train)):\n",
    "    weights_train[y_train == i] *= max(class_weights_train)/ class_weights_train[i] #equalize number of background and signal event\n",
    "    weights_test[y_train == i] *= 1/(1-train_size) # increase test weight to compensate for sampling\n",
    "    \n",
    "print (\"Test : total weight bkg\", weights_test[y_test == 0].sum())\n",
    "print (\"Test : total weight sig\", weights_test[y_test == 1].sum())\n",
    "print (\"Train : total weight bkg\", weights_train[y_train == 0].sum())\n",
    "print (\"Train : total weight sig\", weights_train[y_train == 1].sum())\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxybCOi-rlIM"
   },
   "source": [
    "# Testing NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4DfF0ISrlIN"
   },
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
    "from sklearn.utils import class_weight # to set class_weight=\"balanced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "nnwidth=32\n",
    "Dx = Dense(nnwidth, activation=\"relu\")(inputs)\n",
    "#Dx = Dense(nnwidth, activation=\"relu\")(Dx)\n",
    "#Dx = Dense(nnwidth, activation=\"relu\")(Dx)  # only one layer to save time\n",
    "Dx = Dense(1, activation=\"sigmoid\")(Dx)\n",
    "D = Model(input=[inputs], output=[Dx])\n",
    "D.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "#HPO number of neurones nnwidth, activation function, etc.... check on the web\n",
    "\n",
    "starting_time = time.time( )\n",
    "D.fit(X_train, y_train.values, epochs=10, verbose=0, class_weight=class_weights)\n",
    "training_time = time.time( ) - starting_time\n",
    "print(\"Training time:\",training_time)\n",
    "\n",
    "y_pred_keras = D.predict(X_test)\n",
    "y_pred_keras = y_pred_keras.ravel()\n",
    "y_pred_train_keras = D.predict(X_train).ravel()\n",
    "auc_test_keras = roc_auc_score(y_true=y_test, y_score=y_pred_keras)\n",
    "auc_train_keras = roc_auc_score(y_true=y_train.values, y_score=y_pred_train_keras,)\n",
    "print(\"auc test:\",auc_test_keras)\n",
    "print (\"auc train:\",auc_train_keras)\n",
    "int_pred_test_sig_keras = [weights_test[(y_test ==1) & (y_pred_keras > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "int_pred_test_bkg_keras = [weights_test[(y_test ==0) & (y_pred_keras > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "\n",
    "vamsasimov_keras = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_keras,int_pred_test_bkg_keras)]\n",
    "significance_keras = max(vamsasimov_keras)\n",
    "Z = significance_keras\n",
    "print(\"Z:\",Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE BELOW TO BE ADAPTED XGB=>KERAS (KEEP GBM REFERENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXTRSemqrlIR"
   },
   "outputs": [],
   "source": [
    "#gridSearchCV for advanced HPO, check on the web for other parameters\n",
    "\n",
    "if False:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    #param_list = {'max_depth': [3, 6, 7], 'subsample': [0.7, 1],\n",
    "    #                    'learning_rate': [0.05, 0.3], 'n_estimators': [10, 50, 200]}\n",
    "    param_list_XGB = {'max_depth': [6,10], 'subsample': [0.7,1],\n",
    "                        'learning_rate': [0.05, 0.3], 'max_leaves': [50, 200]}\n",
    "\n",
    "\n",
    "    gsearch1 = GridSearchCV(estimator = XGBClassifier(), \n",
    "    param_grid = param_list_XGB, scoring='roc_auc',n_jobs=4,iid=False, cv=3)\n",
    "    gsearch1.fit(X_train,y_train, weights_train)\n",
    "    print (gsearch1.best_params_)\n",
    "    print (gsearch1.best_score_)\n",
    "\n",
    "    y_pred_gs = gsearch1.predict_proba(X_test)[:,1]\n",
    "    roc_auc_score(y_true=y_test, y_score=y_pred_gs, sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmadcFRJrlIT"
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9YmfuiwUrlIU"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
    "#gbm = lgb.LGBMClassifier()\n",
    "gbm = lgb.LGBMClassifier()\n",
    "# gbm = lgb.LGBMClassifier(max_depth=12) # HPO, check on the web for other parameters\n",
    "\n",
    "\n",
    "starting_time = time.time( )\n",
    "\n",
    "gbm.fit(X_train, y_train.values,sample_weight=weights_train.values)\n",
    "#gbm.fit(X_train, y_train.values) #ma\n",
    "\n",
    "\n",
    "training_time = time.time( ) - starting_time\n",
    "print(\"Training time:\",training_time)\n",
    "\n",
    "y_pred_gbm = gbm.predict_proba(X_test)[:,1]\n",
    "y_pred_gbm = y_pred_gbm.ravel()\n",
    "y_pred_train_gbm = gbm.predict_proba(X_train)[:,1].ravel()\n",
    "auc_test_gbm = roc_auc_score(y_true=y_test, y_score=y_pred_gbm)\n",
    "print(\"auc test:\",auc_test_gbm)\n",
    "print (\"auc train:\",roc_auc_score(y_true=y_train.values, y_score=y_pred_train_gbm,))\n",
    "\n",
    "int_pred_test_sig_gbm = [weights_test[(y_test ==1) & (y_pred_gbm > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "int_pred_test_bkg_gbm = [weights_test[(y_test ==0) & (y_pred_gbm > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "\n",
    "vamsasimov_gbm = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_gbm,int_pred_test_bkg_gbm)]\n",
    "significance_gbm = max(vamsasimov_gbm)\n",
    "Z = significance_gbm\n",
    "print(\"Z:\",Z)\n",
    "# To save model\n",
    "gbm.booster_.save_model(\"LightGBM.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEm5cudSrlIW"
   },
   "outputs": [],
   "source": [
    "#gridSearchCV for advanced HPO\n",
    "if False:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    #param_list_GBM = {'max_depth': [3, 6, 7],\n",
    "    #                     'learning_rate': [0.01, 0.05, 0.1, 0.3, 1], 'n_estimators': [10, 20, 40, 50, 200]}\n",
    "    param_list_GBM = {'max_depth': [6,10], \n",
    "                        'learning_rate': [0.05, 0.3], 'n_estimators': [10, 200]}\n",
    "\n",
    "\n",
    "    gsearch1 = GridSearchCV(estimator = XGBClassifier(), \n",
    "    param_grid = param_list_GBM, scoring='roc_auc',n_jobs=4,iid=False, cv=2)\n",
    "    gsearch1.fit(X_train,y_train, weights_train)\n",
    "    print (gsearch1.best_params_)\n",
    "    print (gsearch1.best_score_)\n",
    "\n",
    "    y_pred_gs = gsearch1.predict_proba(X_test)[:,1]\n",
    "    roc_auc_score(y_true=y_test, y_score=y_pred_gs, sample_weight=weights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmJxYTUmrlIZ"
   },
   "outputs": [],
   "source": [
    "print('Best significance found are:')\n",
    "print('LightGBM: ', significance_gbm)\n",
    "print('XGBoost : ', significance_xgb)\n",
    "print('Best auc train found are:')\n",
    "print('LightGBM: ', roc_auc_score(y_true=y_train.values, y_score=y_pred_train_gbm,))\n",
    "print('XGBoost: ', roc_auc_score(y_true=y_train.values, y_score=y_pred_train_xgb,)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T-LB9cbErlIb"
   },
   "source": [
    "## Some nice plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-a8iJUkrlIb"
   },
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "pdf = PdfPages('LightGBM_XGBoost.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kEQ342s-rlIe"
   },
   "outputs": [],
   "source": [
    "\n",
    "compare_train_test(y_pred_train_xgb, y_train, y_pred_xgb, y_test, xlabel=\"XGboost score\", title=\"XGboost\")#, weights_train=weights_train.values, weights_test=weights_test.values)\n",
    "#plt.savefig(new_dir + \"/Score_BDT_XGBoost_Hist.pdf\")\n",
    "pdf.savefig()\n",
    "plt.show()\n",
    "compare_train_test(y_pred_train_gbm, y_train, y_pred_gbm, y_test, xlabel=\"LightGBM score\", title=\"LightGBM\")#, weights_train=weights_train.values, weights_test=weights_test.values)\n",
    "#plt.savefig(new_dir + \"/Score_BDT_LightGBM.pdf\")\n",
    "pdf.savefig()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tUKIWu2orlIg"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "lw = 2\n",
    "\n",
    "fpr_gbm,tpr_gbm,_ = roc_curve(y_true=y_test, y_score=y_pred_gbm,)#,sample_weight=weights_test.values)\n",
    "fpr_xgb,tpr_xgb,_ = roc_curve(y_true=y_test, y_score=y_pred_xgb,)#,sample_weight=weights_test.values)\n",
    "plt.plot(fpr_gbm, tpr_gbm, color='darkorange',lw=lw, label='LightGBM (AUC  = {})'.format(np.round(auc_test_gbm,decimals=2)))\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='darkgreen',lw=lw, label='XGBoost (AUC  = {})'.format(np.round(auc_test_xgb,decimals=2)))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#import os\n",
    "#new_dir = \"Plots/Comparing\" \n",
    "#if not os.path.isdir(new_dir):\n",
    "#    os.mkdir(new_dir)\n",
    "#plt.savefig(new_dir + \"/ROC_comparing.pdf\")\n",
    "pdf.savefig()\n",
    "plt.show() # blue line = random classification -> maximize true positive rate while miniize false positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGF3k0KJrlIi"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),vamsasimov_gbm, label='LightGBM (Z = {})'.format(np.round(significance_gbm,decimals=2)))\n",
    "plt.plot(np.linspace(0,1,num=50),vamsasimov_xgb, label='XGBoost (Z = {})'.format(np.round(significance_xgb,decimals=2)))\n",
    "\n",
    "plt.title(\"BDT Significance\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Significance\")\n",
    "plt.legend()\n",
    "#plt.savefig(new_dir + \"/Significance_comparing.pdf\")\n",
    "pdf.savefig()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCxwGoP7rlIk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QN2h9y5JrlIm"
   },
   "outputs": [],
   "source": [
    "plt.bar(data.columns.values, xgb.feature_importances_)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Feature importances XGBoost Hist\")\n",
    "#plt.savefig(new_dir + \"/VarImp_BDT_XGBoost_Hist.pdf\",bbox_inches='tight')\n",
    "#pdf.savefig(bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.bar(data.columns.values, gbm.feature_importances_)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Feature importances LightGBM\")\n",
    "#plt.savefig(new_dir + \"/VarImp_BDT_LightGBM.pdf\",bbox_inches='tight')\n",
    "#pdf.savefig(bbox_inches='tight')\n",
    "plt.show()\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7WxH3bDrlIp"
   },
   "source": [
    "# Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sMzhd_vWrlIq"
   },
   "outputs": [],
   "source": [
    "#a bit slow\n",
    "#!pip install eli5\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "file = open('html_original.html', 'w')\n",
    "file.write(HTML('<h1>Permutation importances XGBoost</h1>').data)\n",
    "perm_xgb = PermutationImportance(xgb, random_state=1).fit(X_test, y_test)#, sample_weight=weights_test.values)\n",
    "html_xgb = eli5.show_weights(perm_xgb, feature_names = data.columns.values).data\n",
    "#with open('html_xgb.html', 'w') as f:\n",
    "#    f.write(HTML('<h1>Permutation importances XGBoost Hist</h1>').data)\n",
    "#    f.write(html_xgb)\n",
    "file.write(html_xgb)\n",
    "perm_gbm = PermutationImportance(gbm, random_state=1).fit(X_test, y_test)#, sample_weight=weights_test.values)\n",
    "html_gbm = eli5.show_weights(perm_gbm, feature_names = data.columns.values).data\n",
    "#with open('html_gbm.html', 'w') as f:\n",
    "#    f.write(HTML('<h1>Permutation importances LightGBM</h1>').data)\n",
    "#    f.write(html_gbm)\n",
    "file.write(HTML('<h1>Permutation importances LightGBM</h1>').data)\n",
    "file.write(html_gbm)\n",
    "print (\"Permutation importances XGBoost\")\n",
    "display(eli5.show_weights(perm_xgb, feature_names = data.columns.values))\n",
    "print (\"Permutation importances LightGBM\")\n",
    "display(eli5.show_weights(perm_gbm, feature_names = data.columns.values))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erzHgnnorlIs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de ST4PNL_ML_TD_BDT.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
