{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de ST4PNL_ML_TD_BDT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT7-MMpfrlHR",
        "colab_type": "text"
      },
      "source": [
        "# Standard Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdmaHjz-xCm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "print (np.__version__)\n",
        "!pip install numpy==1.15.4 # to avoid bug loading hdf file \"ValueError: cannot set WRITEABLE flag to True of this array\"\n",
        "print (np.__version__)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QkqebuVzm_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsChk6u-r7fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Reading file from Google Drive\n",
        "!pip install PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl-4W4Ifs8EV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download dataset \n",
        "#directory download = drive.CreateFile({'id': '1TuvyEky9lCcc5ybC6nTl5zzkIm8p8AVO'})\n",
        "#download.GetContentFile(\"dataWW\")\n",
        "\n",
        "#download three file \n",
        "download = drive.CreateFile({'id': '1RN6BVBH4A3v_BBKguiEDJQ2z_pLNWHL_'})\n",
        "download.GetContentFile(\"dataWW_qq.hdf\")\n",
        "download = drive.CreateFile({'id': '12kRnrvsluiBvXqvNOk2lb0Agna13lSzV'})\n",
        "download.GetContentFile(\"dataWW_ggH.hdf\")\n",
        "download = drive.CreateFile({'id': '1nsGp13Mw3CzjtON---j1cWQziuFpkoEp'})\n",
        "download.GetContentFile(\"dataWW_VBFH.hdf\")\n",
        "\n",
        "\n",
        "\n",
        "!ls -lrt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDGbfWaTrlHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#from IPython import display\n",
        "from IPython.display import display, HTML\n",
        "%matplotlib inline\n",
        "import time\n",
        "pd.set_option('display.max_columns', None) # to see all columns of df.head()\n",
        "np.random.seed(31415) # set the random seed\n",
        "\n",
        "#!pip install pytables # to read hdf5\n",
        "#!pip install xgboost\n",
        "#!pip install lightgbm # not sure it loads the .so\n",
        "#!conda install --yes lightgbm # better run in separate window"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSRkP7FRr27Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVJFiNmgrlHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some utilities\n",
        "from math import sqrt\n",
        "from math import log\n",
        "\n",
        "\n",
        "def amsasimov(s,b): # asimove significance\n",
        "        if b<=0 or s<=0:\n",
        "            return 0\n",
        "        try:\n",
        "            return sqrt(2*((s+b)*log(1+float(s)/b)-s))\n",
        "        except ValueError:\n",
        "            print(1+float(s)/b)\n",
        "            print (2*((s+b)*log(1+float(s)/b)-s))\n",
        "        #return s/sqrt(s+b)\n",
        "\n",
        "#\n",
        "def compare_train_test(y_pred_train, y_train, y_pred, y_test, high_low=(0,1), bins=30, xlabel=\"\", ylabel=\"Arbitrary units\", title=\"\", weights_train=np.array([]), weights_test=np.array([])):\n",
        "    if weights_train.size != 0:\n",
        "        weights_train_signal = weights_train[y_train == 1]\n",
        "        weights_train_background = weights_train[y_train == 0]\n",
        "    else:\n",
        "        weights_train_signal = None\n",
        "        weights_train_background = None\n",
        "    plt.hist(y_pred_train[y_train == 1],\n",
        "                 color='r', alpha=0.5, range=high_low, bins=bins,\n",
        "                 histtype='stepfilled', normed=True,\n",
        "                 label='S (train)', weights=weights_train_signal) # alpha is transparancy\n",
        "    plt.hist(y_pred_train[y_train == 0],\n",
        "                 color='b', alpha=0.5, range=high_low, bins=bins,\n",
        "                 histtype='stepfilled', normed=True,\n",
        "                 label='B (train)', weights=weights_train_background)\n",
        "\n",
        "    if weights_test.size != 0:\n",
        "        weights_test_signal = weights_test[y_test == 1]\n",
        "        weights_test_background = weights_test[y_test == 0]\n",
        "    else:\n",
        "        weights_test_signal = None\n",
        "        weights_test_background = None\n",
        "    hist, bins = np.histogram(y_pred[y_test == 1],\n",
        "                                  bins=bins, range=high_low, normed=True, weights=weights_test_signal)\n",
        "    scale = len(y_pred[y_test == 1]) / sum(hist)\n",
        "    err = np.sqrt(hist * scale) / scale\n",
        "\n",
        "    #width = (bins[1] - bins[0])\n",
        "    center = (bins[:-1] + bins[1:]) / 2\n",
        "    plt.errorbar(center, hist, yerr=err, fmt='o', c='r', label='S (test)')\n",
        "\n",
        "    hist, bins = np.histogram(y_pred[y_test == 0],\n",
        "                                  bins=bins, range=high_low, normed=True, weights=weights_test_background)\n",
        "    scale = len(y_pred[y_test == 0]) / sum(hist)\n",
        "    err = np.sqrt(hist * scale) / scale\n",
        "\n",
        "    #width = (bins[1] - bins[0])\n",
        "    center = (bins[:-1] + bins[1:]) / 2\n",
        "    plt.errorbar(center, hist, yerr=err, fmt='o', c='b', label='B (test)')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend(loc='best')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBqFUzeXrlHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load signal, background data\n",
        "#more info on dataset http://opendata.atlas.cern/books/current/openatlasdatatools/_book/simulated_data_details.html\n",
        "# dataset ID qq : 105985, ggH : 160155, vbfh : 160205\n",
        "# cross section  qq: 12.42 pb, ggH: 13.17 pb , VBFH : 1.617 pb\n",
        "# unfortunately cross section in the document are wrong\n",
        "qq_cross_section=1242 # deliberately wrong to increase background level\n",
        "ggh_cross_section=13.17*0.5 # deliberately wrong to \n",
        "vbfh_cross_section=1.617*0.5 # deliberately wrong to \n",
        "\n",
        "\n",
        "\n",
        "# total integrated luminosity ATLAS 2012 : fb^-1\n",
        "luminosity=21 \n",
        "# even more info in http://opendata.atlas.cern/books/current/openatlasdatatools/_book/glossary.html \n",
        "\n",
        "#if fail to load, might be due to numpy version, see first cell\n",
        "qq_events = pd.read_hdf(\"dataWW_qq.hdf\",\"qq\",WRITEABLE=False ) #quarks (qq background) http://opendata.cern.ch/record/3800\n",
        "ggh_events = pd.read_hdf(\"dataWW_ggH.hdf\",\"ggH\",WRITEABLE=False ) # gluons fusion http://opendata.cern.ch/record/3825\n",
        "vbfh_events = pd.read_hdf(\"dataWW_VBFH.hdf\",\"VBFH\",WRITEABLE=False ) #Vector Boson Fusion http://opendata.cern.ch/record/3826\n",
        "vbfh_events.columns\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgfAFqAqrlHX",
        "colab_type": "raw"
      },
      "source": [
        "# load signal, background data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzfF0gNlrlHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nvbfkeep=int(ggh_events.shape[0]*0.1232)\n",
        "print (\"keep only \",nvbfkeep, \" vbfh events to respect vbf H/ gg H ratio\")\n",
        "vbfh_events=vbfh_events[0:nvbfkeep]\n",
        "\n",
        "\n",
        "qq_events[\"class\"] = 0   # background category\n",
        "ggh_events[\"class\"] = 1  # signal category\n",
        "vbfh_events[\"class\"] = 1  # signal category\n",
        "h_events = pd.concat([ggh_events, vbfh_events]) # merge the two higgs dataset\n",
        "del ggh_events  # avoid mistakes later\n",
        "del vbfh_events  # avoid mistakes later"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9l7wkkorlHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h_events.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz-lWJhgrlHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h_events.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQsalTmorlHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "vbfh_cross_section=1.617\n",
        "h_cross_section=ggh_cross_section+vbfh_cross_section\n",
        "\n",
        "\n",
        "print (\"before normalisation\")\n",
        "# in principle should multiply all the scaleFactor (then weights will vary event by event)\n",
        "class_weights = (qq_events.mcWeight.sum(), h_events.mcWeight.sum()) \n",
        "print(\"total class weights\",class_weights)\n",
        "\n",
        "\n",
        "class_nevents = (len(qq_events.index), len(h_events.index))\n",
        "print (\"total class number of events\",class_nevents)\n",
        "\n",
        "qq_weight=qq_cross_section*luminosity/qq_events.shape[0]*qq_events.mcWeight\n",
        "qq_events[\"weight\"]=qq_weight\n",
        "h_weight=h_cross_section*luminosity/h_events.shape[0]*h_events.mcWeight\n",
        "h_events[\"weight\"]=h_weight\n",
        "\n",
        "print (\"after normalisation\")\n",
        "# in principle should multiply all the scaleFactor (then weights will vary event by event)\n",
        "total_weights = (qq_events.weight.sum(), h_events.weight.sum()) \n",
        "\n",
        "print(\"total total weights\",total_weights)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nnlsmXOrlHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fulldata = pd.concat([qq_events, h_events]) # merge the datasets\n",
        "fulldata = fulldata.sample(frac=1).reset_index(drop=True) #shuffle the events\n",
        "fulldata.head(10)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtI5u5GErlHq",
        "colab_type": "text"
      },
      "source": [
        "## Event selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaO2JM1hrlHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (fulldata.shape)\n",
        "fulldata=fulldata[fulldata.lep_n==2]\n",
        "print (fulldata.shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CrdDqZgrlHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fulldata.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5tCZ6cTrlHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DR replace dummy -999 by -10 because of XGboost hist bug\n",
        "fulldata.replace(-999,-7,inplace=True)\n",
        "fulldata.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ksUkHZprlHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMQpKhDKrlH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hide class in separate vector\n",
        "#WARNING : there should be no selection nor shuffling later on !\n",
        "target = fulldata[\"class\"]\n",
        "del fulldata[\"class\"]\n",
        "\n",
        "#hide weight in separate vector\n",
        "weights = fulldata[\"weight\"]\n",
        "del fulldata[\"weight\"]\n",
        "fulldata.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nviyIMgerlH3",
        "colab_type": "text"
      },
      "source": [
        "# DO NOT MODIFY ANYTHING ABOVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6e0Hlpv6rlH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# always rerun exercises from this cell\n",
        "# for simplicity of the exercise only keep some features\n",
        "data=fulldata[[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_phi_0', 'lep_phi_1','jet_n','jet_eta_0', 'jet_eta_1']]\n",
        "#data=fulldata[[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_eta_0', 'lep_eta_1', 'lep_phi_0', 'lep_phi_1','jet_n','jet_pt_0',\n",
        "#       'jet_pt_1', 'jet_eta_0', 'jet_eta_1', 'jet_phi_0', 'jet_phi_1']]\n",
        "print (data.shape)\n",
        "data.head()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2mf1bLVrlH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "data[target==0].hist(figsize=(15,12),color='b')\n",
        "data[target==1].hist(figsize=(15,12),color='r')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZM-E5H4rlH-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvGal8xwzKUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzNVhZidzKwr",
        "colab_type": "text"
      },
      "source": [
        "# Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBSy1alNrlH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#data[\"lep_deltaphi\"]=np.abs(np.mod(data[\"lep_phi_0\"]-data[\"lep_phi_1\"]+3*np.pi,2*np.pi)-np.pi)\n",
        "#data[\"lep_deltaphi\"]=data[\"lep_phi_0\"]-data[\"lep_phi_1\"]\n",
        "\n",
        "print (data.shape)\n",
        "data.head()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kowHjX4rlIC",
        "colab_type": "text"
      },
      "source": [
        "## Transformation of the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9j5hdrmrlID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "train_size = 0.75 # fraction of sample used for training\n",
        "X_train, X_test, y_train, y_test, weights_train, weights_test = \\\n",
        "    train_test_split(data, target, weights, train_size=train_size)\n",
        "#reset index for dataseries, not needed for ndarray (X_train, X_test)\n",
        "y_train, y_test, weights_train, weights_test = \\\n",
        "    y_train.reset_index(drop=True),y_test.reset_index(drop=True), \\\n",
        "    weights_train.reset_index(drop=True), weights_test.reset_index(drop=True)\n",
        "print (X_train.shape)\n",
        "print (y_train.shape)\n",
        "print (weights_train.shape)\n",
        "print (X_test.shape)\n",
        "print (y_test.shape)\n",
        "print (weights_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#scale to mean 0 and variance 1\n",
        "scaler = StandardScaler()\n",
        "#scaler.fit(data)\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)  # applied the transformation calculated the line above\n",
        "\n",
        "\n",
        "class_weights_train = (weights_train[y_train == 0].sum(), weights_train[y_train == 1].sum())\n",
        "print (\"class_weights_train:\",class_weights_train)\n",
        "for i in range(len(class_weights_train)):\n",
        "    weights_train[y_train == i] *= max(class_weights_train)/ class_weights_train[i] #equalize number of background and signal event\n",
        "    weights_test[y_train == i] *= 1/(1-train_size) # increase test weight to compensate for sampling\n",
        "    \n",
        "print (\"Test : total weight bkg\", weights_test[y_test == 0].sum())\n",
        "print (\"Test : total weight sig\", weights_test[y_test == 1].sum())\n",
        "print (\"Train : total weight bkg\", weights_train[y_train == 0].sum())\n",
        "print (\"Train : total weight sig\", weights_train[y_train == 1].sum())\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxybCOi-rlIM",
        "colab_type": "text"
      },
      "source": [
        "# Testing BDT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4DfF0ISrlIN",
        "colab_type": "text"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "nqMCgvbkrlIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(31415) # set the random seed\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
        "xgb = XGBClassifier(tree_method=\"hist\")\n",
        "#xgb = XGBClassifier(tree_method=\"hist\",max_depth=12) # HPO, check on the web for other parameters\n",
        "# not a bad idea to check for bugs without hist\n",
        "\n",
        "\n",
        "starting_time = time.time( )\n",
        "xgb.fit(X_train, y_train.values, sample_weight=weights_train.values)\n",
        "training_time = time.time( ) - starting_time\n",
        "print(\"Training time:\",training_time)\n",
        "\n",
        "y_pred_xgb = xgb.predict_proba(X_test)[:,1]\n",
        "y_pred_xgb = y_pred_xgb.ravel()\n",
        "y_pred_train_xgb = xgb.predict_proba(X_train)[:,1].ravel()\n",
        "auc_test_xgb = roc_auc_score(y_true=y_test, y_score=y_pred_xgb)\n",
        "print(\"auc test:\",auc_test_xgb)\n",
        "print (\"auc train:\",roc_auc_score(y_true=y_train.values, y_score=y_pred_train_xgb,))\n",
        "\n",
        "int_pred_test_sig_xgb = [weights_test[(y_test ==1) & (y_pred_xgb > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "int_pred_test_bkg_xgb = [weights_test[(y_test ==0) & (y_pred_xgb > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "\n",
        "vamsasimov_xgb = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_xgb,int_pred_test_bkg_xgb)]\n",
        "significance_xgb = max(vamsasimov_xgb)\n",
        "Z = significance_xgb\n",
        "print(\"Z:\",Z)\n",
        "# To save model\n",
        "xgb.save_model(\"XGBoost.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXTRSemqrlIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#gridSearchCV for advanced HPO, check on the web for other parameters\n",
        "\n",
        "if False:\n",
        "    from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "    #param_list = {'max_depth': [3, 6, 7], 'subsample': [0.7, 1],\n",
        "    #                    'learning_rate': [0.05, 0.3], 'n_estimators': [10, 50, 200]}\n",
        "    param_list_XGB = {'max_depth': [6,10], 'subsample': [0.7,1],\n",
        "                        'learning_rate': [0.05, 0.3], 'max_leaves': [50, 200]}\n",
        "\n",
        "\n",
        "    gsearch1 = GridSearchCV(estimator = XGBClassifier(), \n",
        "    param_grid = param_list_XGB, scoring='roc_auc',n_jobs=4,iid=False, cv=3)\n",
        "    gsearch1.fit(X_train,y_train, weights_train)\n",
        "    print (gsearch1.best_params_)\n",
        "    print (gsearch1.best_score_)\n",
        "\n",
        "    y_pred_gs = gsearch1.predict_proba(X_test)[:,1]\n",
        "    roc_auc_score(y_true=y_test, y_score=y_pred_gs, sample_weight=weights_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmadcFRJrlIT",
        "colab_type": "text"
      },
      "source": [
        "## LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YmfuiwUrlIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
        "#gbm = lgb.LGBMClassifier()\n",
        "gbm = lgb.LGBMClassifier()\n",
        "# gbm = lgb.LGBMClassifier(max_depth=12) # HPO, check on the web for other parameters\n",
        "\n",
        "\n",
        "starting_time = time.time( )\n",
        "\n",
        "gbm.fit(X_train, y_train.values,sample_weight=weights_train.values)\n",
        "#gbm.fit(X_train, y_train.values) #ma\n",
        "\n",
        "\n",
        "training_time = time.time( ) - starting_time\n",
        "print(\"Training time:\",training_time)\n",
        "\n",
        "y_pred_gbm = gbm.predict_proba(X_test)[:,1]\n",
        "y_pred_gbm = y_pred_gbm.ravel()\n",
        "y_pred_train_gbm = gbm.predict_proba(X_train)[:,1].ravel()\n",
        "auc_test_gbm = roc_auc_score(y_true=y_test, y_score=y_pred_gbm)\n",
        "print(\"auc test:\",auc_test_gbm)\n",
        "print (\"auc train:\",roc_auc_score(y_true=y_train.values, y_score=y_pred_train_gbm,))\n",
        "\n",
        "int_pred_test_sig_gbm = [weights_test[(y_test ==1) & (y_pred_gbm > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "int_pred_test_bkg_gbm = [weights_test[(y_test ==0) & (y_pred_gbm > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "\n",
        "vamsasimov_gbm = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_gbm,int_pred_test_bkg_gbm)]\n",
        "significance_gbm = max(vamsasimov_gbm)\n",
        "Z = significance_gbm\n",
        "print(\"Z:\",Z)\n",
        "# To save model\n",
        "gbm.booster_.save_model(\"LightGBM.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEm5cudSrlIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#gridSearchCV for advanced HPO\n",
        "if False:\n",
        "    from sklearn.model_selection import GridSearchCV\n",
        "    #param_list_GBM = {'max_depth': [3, 6, 7],\n",
        "    #                     'learning_rate': [0.01, 0.05, 0.1, 0.3, 1], 'n_estimators': [10, 20, 40, 50, 200]}\n",
        "    param_list_GBM = {'max_depth': [6,10], \n",
        "                        'learning_rate': [0.05, 0.3], 'n_estimators': [10, 200]}\n",
        "\n",
        "\n",
        "    gsearch1 = GridSearchCV(estimator = XGBClassifier(), \n",
        "    param_grid = param_list_GBM, scoring='roc_auc',n_jobs=4,iid=False, cv=2)\n",
        "    gsearch1.fit(X_train,y_train, weights_train)\n",
        "    print (gsearch1.best_params_)\n",
        "    print (gsearch1.best_score_)\n",
        "\n",
        "    y_pred_gs = gsearch1.predict_proba(X_test)[:,1]\n",
        "    roc_auc_score(y_true=y_test, y_score=y_pred_gs, sample_weight=weights_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmJxYTUmrlIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Best significance found are:')\n",
        "print('LightGBM: ', significance_gbm)\n",
        "print('XGBoost : ', significance_xgb)\n",
        "print('Best auc train found are:')\n",
        "print('LightGBM: ', roc_auc_score(y_true=y_train.values, y_score=y_pred_train_gbm,))\n",
        "print('XGBoost: ', roc_auc_score(y_true=y_train.values, y_score=y_pred_train_xgb,)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-LB9cbErlIb",
        "colab_type": "text"
      },
      "source": [
        "## Some nice plots "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-a8iJUkrlIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "pdf = PdfPages('LightGBM_XGBoost.pdf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEQ342s-rlIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "compare_train_test(y_pred_train_xgb, y_train, y_pred_xgb, y_test, xlabel=\"XGboost score\", title=\"XGboost\")#, weights_train=weights_train.values, weights_test=weights_test.values)\n",
        "#plt.savefig(new_dir + \"/Score_BDT_XGBoost_Hist.pdf\")\n",
        "pdf.savefig()\n",
        "plt.show()\n",
        "compare_train_test(y_pred_train_gbm, y_train, y_pred_gbm, y_test, xlabel=\"LightGBM score\", title=\"LightGBM\")#, weights_train=weights_train.values, weights_test=weights_test.values)\n",
        "#plt.savefig(new_dir + \"/Score_BDT_LightGBM.pdf\")\n",
        "pdf.savefig()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUKIWu2orlIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "lw = 2\n",
        "\n",
        "fpr_gbm,tpr_gbm,_ = roc_curve(y_true=y_test, y_score=y_pred_gbm,)#,sample_weight=weights_test.values)\n",
        "fpr_xgb,tpr_xgb,_ = roc_curve(y_true=y_test, y_score=y_pred_xgb,)#,sample_weight=weights_test.values)\n",
        "plt.plot(fpr_gbm, tpr_gbm, color='darkorange',lw=lw, label='LightGBM (AUC  = {})'.format(np.round(auc_test_gbm,decimals=2)))\n",
        "plt.plot(fpr_xgb, tpr_xgb, color='darkgreen',lw=lw, label='XGBoost (AUC  = {})'.format(np.round(auc_test_xgb,decimals=2)))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "#import os\n",
        "#new_dir = \"Plots/Comparing\" \n",
        "#if not os.path.isdir(new_dir):\n",
        "#    os.mkdir(new_dir)\n",
        "#plt.savefig(new_dir + \"/ROC_comparing.pdf\")\n",
        "pdf.savefig()\n",
        "plt.show() # blue line = random classification -> maximize true positive rate while miniize false positive rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGF3k0KJrlIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(np.linspace(0,1,num=50),vamsasimov_gbm, label='LightGBM (Z = {})'.format(np.round(significance_gbm,decimals=2)))\n",
        "plt.plot(np.linspace(0,1,num=50),vamsasimov_xgb, label='XGBoost (Z = {})'.format(np.round(significance_xgb,decimals=2)))\n",
        "\n",
        "plt.title(\"BDT Significance\")\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"Significance\")\n",
        "plt.legend()\n",
        "#plt.savefig(new_dir + \"/Significance_comparing.pdf\")\n",
        "pdf.savefig()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCxwGoP7rlIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN2h9y5JrlIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.bar(data.columns.values, xgb.feature_importances_)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Feature importances XGBoost Hist\")\n",
        "#plt.savefig(new_dir + \"/VarImp_BDT_XGBoost_Hist.pdf\",bbox_inches='tight')\n",
        "#pdf.savefig(bbox_inches='tight')\n",
        "plt.show()\n",
        "plt.bar(data.columns.values, gbm.feature_importances_)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Feature importances LightGBM\")\n",
        "#plt.savefig(new_dir + \"/VarImp_BDT_LightGBM.pdf\",bbox_inches='tight')\n",
        "#pdf.savefig(bbox_inches='tight')\n",
        "plt.show()\n",
        "pdf.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7WxH3bDrlIp",
        "colab_type": "text"
      },
      "source": [
        "# Permutation importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMzhd_vWrlIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#a bit slow\n",
        "#!pip install eli5\n",
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "file = open('html_original.html', 'w')\n",
        "file.write(HTML('<h1>Permutation importances XGBoost</h1>').data)\n",
        "perm_xgb = PermutationImportance(xgb, random_state=1).fit(X_test, y_test)#, sample_weight=weights_test.values)\n",
        "html_xgb = eli5.show_weights(perm_xgb, feature_names = data.columns.values).data\n",
        "#with open('html_xgb.html', 'w') as f:\n",
        "#    f.write(HTML('<h1>Permutation importances XGBoost Hist</h1>').data)\n",
        "#    f.write(html_xgb)\n",
        "file.write(html_xgb)\n",
        "perm_gbm = PermutationImportance(gbm, random_state=1).fit(X_test, y_test)#, sample_weight=weights_test.values)\n",
        "html_gbm = eli5.show_weights(perm_gbm, feature_names = data.columns.values).data\n",
        "#with open('html_gbm.html', 'w') as f:\n",
        "#    f.write(HTML('<h1>Permutation importances LightGBM</h1>').data)\n",
        "#    f.write(html_gbm)\n",
        "file.write(HTML('<h1>Permutation importances LightGBM</h1>').data)\n",
        "file.write(html_gbm)\n",
        "print (\"Permutation importances XGBoost\")\n",
        "display(eli5.show_weights(perm_xgb, feature_names = data.columns.values))\n",
        "print (\"Permutation importances LightGBM\")\n",
        "display(eli5.show_weights(perm_gbm, feature_names = data.columns.values))\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erzHgnnorlIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}